import streamlit as st
import json
import torch
import gc
import numpy as np
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# ==========================================================
# üé® UI
# ==========================================================
st.set_page_config(page_title="RAG Hybrid Chatbot", page_icon="üéì", layout="wide")
st.title("Tr·ª£ l√Ω Quy ch·∫ø Sinh vi√™n")

# ==========================================================
# üìÇ LOAD RESOURCES
# ==========================================================
@st.cache_resource
def load_resources():
    status = st.sidebar.empty()
    status.info("‚è≥ ƒêang load t√†i nguy√™n...")

    with open("chunking_file.json", "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    status.info("‚è≥ K·∫øt n·ªëi ChromaDB...")
    embedding = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large",
        model_kwargs={"device": "cuda"} # ƒê·ªïi th√†nh "cpu" n·∫øu kh√¥ng c√≥ card r·ªùi
    )

    vector_db = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embedding
    )

    status.info("‚è≥ Kh·ªüi t·∫°o BM25...")
    tokenized = [doc.lower().split() for doc in raw_data]
    bm25 = BM25Okapi(tokenized)

    status.info("‚è≥ Load Reranker...")
    reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device="cuda")

    status.info("‚è≥ Load LLM Qwen 7B...")
    
    # L∆ØU √ù CHO B·∫†N: S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n n√†y. 
    # N·∫øu mu·ªën m√°y t·ª± t·∫£i model t·ª´ m·∫°ng, h√£y ƒë·ªïi th√†nh: MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
    MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"

    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb,
        device_map="auto",
        trust_remote_code=True
    )

    status.success("‚úÖ S·∫µn s√†ng!")
    return raw_data, vector_db, bm25, reranker, tokenizer, model


raw_data, vector_db, bm25, reranker, tokenizer, model = load_resources()

# ==========================================================
# üîç HYBRID SEARCH
# ==========================================================
def hybrid_search(query, top_k=3):
    vec_docs = vector_db.similarity_search(query, k=10)
    vec_texts = [d.page_content for d in vec_docs]

    tokens = query.lower().split()
    bm25_texts = bm25.get_top_n(tokens, raw_data, n=10)

    candidates = list(set(vec_texts + bm25_texts))
    if not candidates:
        return []

    scores = reranker.predict([[query, c] for c in candidates])
    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    return [c[0] for c in ranked[:top_k]]

# ==========================================================
# ü§ñ GENERATE RESPONSE
# ==========================================================
def generate_response(query):
    ctx = "\n\n".join(hybrid_search(query))

    messages = [
        {
            "role": "system",
            "content":
            "B·∫°n l√† **Tr·ª£ l√Ω ·∫£o d√†nh cho sinh vi√™n Tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc ‚Äì ƒê·∫°i h·ªçc Hu·∫ø**.\n\n"
            "Nhi·ªám v·ª• c·ªßa b·∫°n:\n"
            "1. Lu√¥n tr·∫£ l·ªùi d·ª±a tr√™n **‚ÄúTh√¥ng tin tham kh·∫£o‚Äù**.\n"
            "2. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong t√†i li·ªáu.\n"
            "3. N·∫øu KH√îNG c√≥ th√¥ng tin, tr·∫£ l·ªùi:\n"
            "‚ÄúXin l·ªói, th√¥ng tin n√†y kh√¥ng n·∫±m trong t√†i li·ªáu ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p. T√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi.‚Äù\n"
            "4. Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa.\n"
            "5. T·ªïng h·ª£p ch√≠nh x√°c, d·ªÖ hi·ªÉu.\n"
            "6. Kh√¥ng tr·∫£ l·ªùi ngo√†i ph·∫°m vi t√†i li·ªáu.\n"
        },
        {
            "role": "user",
            "content": f"Th√¥ng tin tham kh·∫£o:\n{ctx}\n\nC√¢u h·ªèi: {query}"
        }
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    outputs = model.generate(**inputs, max_new_tokens=600, temperature=0.3)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return answer.split("assistant\n")[-1]

# ==========================================================
# üí¨ CHAT UI
# ==========================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if st.sidebar.button("üßπ X√≥a l·ªãch s·ª≠"):
    st.session_state.messages = []
    st.rerun()

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            answer = generate_response(prompt)
            st.markdown(answer)

    st.session_state.messages.append({"role": "assistant", "content": answer})
    torch.cuda.empty_cache()
    gc.collect()